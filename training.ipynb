{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "from os import path\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(in_path):\n",
    "    \"\"\"\n",
    "    Creates new features from different columns and exports to csv file.\n",
    "    \"\"\"\n",
    "    # Create alone and baby columns\n",
    "    dataset = pd.read_csv(in_path)\n",
    "\n",
    "    dataset[\"Alone\"] = np.logical_and(\n",
    "        (dataset[\"SibSp\"].eq(0)), (dataset[\"Parch\"].eq(0))\n",
    "    ).astype(int)\n",
    "    dataset[\"Baby\"] = dataset[\"Age\"].le(6).astype(int)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variáveis e Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data\"\n",
    "RAW_FOLDER = \"raw\"\n",
    "PROCESSED_FOLDER = \"processed\"\n",
    "OUTPUT_FOLDER = \"output\"\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"\n",
    "\n",
    "input_train = path.join(DATA_FOLDER, RAW_FOLDER, TRAIN_FILE)\n",
    "input_test = path.join(DATA_FOLDER, RAW_FOLDER, TEST_FILE)\n",
    "\n",
    "# Prepare pipeline steps.\n",
    "# Transformer for categorical features\n",
    "categorical_features = [\"Pclass\", \"Sex\"]\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        (\"imputer_cat\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Transformer for numerical features.\n",
    "# We use MinMaxScaler because of our data distribution\n",
    "numeric_features = [\"Age\", \"Fare\"]\n",
    "numeric_transformer = Pipeline(\n",
    "    [(\"imputer_num\", SimpleImputer(strategy=\"median\")), (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "# Transformer for numerical to categorical features\n",
    "num_cat_features = [\"Alone\", \"Baby\"]\n",
    "num_cat_transformer = Pipeline(\n",
    "    [\n",
    "        (\"imputer_num_cat\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"categoricals\", categorical_transformer, categorical_features),\n",
    "        (\"numericals\", numeric_transformer, numeric_features),\n",
    "        (\"num_to_cat\", num_cat_transformer, num_cat_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ingestão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest datasets\n",
    "\n",
    "train = get_dataset(input_train, \"train\")\n",
    "test = get_dataset(input_test, \"test\")\n",
    "\n",
    "X = train.drop(\"Survived\", axis=1)\n",
    "y = train[\"Survived\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis e parametros\n",
    "\n",
    "lr_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\"lr\", LogisticRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "lr_params = {\n",
    "    \"lr__penalty\": [\"l1\", \"l2\"],\n",
    "    \"lr__C\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"lr__random_state\": [42],\n",
    "    \"lr__solver\": [\"liblinear\"],\n",
    "}\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "\n",
    "lr_cv = GridSearchCV(\n",
    "    lr_pipeline,\n",
    "    lr_params,\n",
    "    cv=rskf,\n",
    "    scoring=[\"f1\", \"accuracy\"],\n",
    "    refit=\"f1\",\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1-score: 0.726\n",
      "\n",
      "Best parameter set: {'lr__C': 1, 'lr__penalty': 'l1', 'lr__random_state': 42, 'lr__solver': 'liblinear'}\n",
      "\n",
      "Scores:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       549\n",
      "           1       0.77      0.71      0.74       342\n",
      "\n",
      "    accuracy                           0.81       891\n",
      "   macro avg       0.80      0.79      0.79       891\n",
      "weighted avg       0.81      0.81      0.81       891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_cv.fit(X, y)\n",
    "print(f\"Best F1-score: {lr_cv.best_score_:.3f}\\n\")\n",
    "print(f\"Best parameter set: {lr_cv.best_params_}\\n\")\n",
    "print(f\"Scores: {classification_report(y, lr_cv.predict(X))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis e parametros\n",
    "\n",
    "rf_pipeline = Pipeline(\n",
    "    [(\"preprocessing\", preprocessor), (\"rf\", RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "rf_params = {\n",
    "    \"rf__n_estimators\": [100, 120, 150],\n",
    "    \"rf__criterion\": [\"entropy\", \"gini\"],\n",
    "    \"rf__max_depth\": [4, 5, 6],\n",
    "    \"rf__min_samples_leaf\": [0.05, 0.1, 0.2],\n",
    "    \"rf__min_samples_split\": [0.05, 0.1, 0.2],\n",
    "    \"rf__random_state\": [42],\n",
    "}\n",
    "\n",
    "rf_cv = GridSearchCV(\n",
    "    rf_pipeline,\n",
    "    rf_params,\n",
    "    cv=rskf,\n",
    "    scoring=[\"f1\", \"accuracy\"],\n",
    "    refit=\"f1\",\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1-score: 0.710\n",
      "\n",
      "Best parameter set: {'rf__criterion': 'entropy', 'rf__max_depth': 4, 'rf__min_samples_leaf': 0.2, 'rf__min_samples_split': 0.05, 'rf__n_estimators': 120, 'rf__random_state': 42}\n",
      "\n",
      "Scores:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83       549\n",
      "           1       0.74      0.68      0.71       342\n",
      "\n",
      "    accuracy                           0.79       891\n",
      "   macro avg       0.78      0.77      0.77       891\n",
      "weighted avg       0.78      0.79      0.78       891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_cv.fit(X, y)\n",
    "print(f\"Best F1-score: {rf_cv.best_score_:.3f}\\n\")\n",
    "print(f\"Best parameter set: {rf_cv.best_params_}\\n\")\n",
    "print(f\"Scores: {classification_report(y, rf_cv.predict(X))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis e parametros\n",
    "\n",
    "ada_pipeline = Pipeline(\n",
    "    [(\"preprocessing\", preprocessor), (\"ada\", AdaBoostClassifier())]\n",
    ")\n",
    "\n",
    "# Usando alguns dos parametros encontrados antes.\n",
    "ada_params = {\n",
    "    'ada__estimator': [\n",
    "        LogisticRegression(penalty='l2', C=1, random_state=42, solver='liblinear'),\n",
    "        DecisionTreeClassifier(\n",
    "            criterion='gini', max_depth=5, min_samples_leaf=0.05,\n",
    "            min_samples_split=0.2, random_state=42\n",
    "        ),\n",
    "    ],\n",
    "    'ada__n_estimators': [30, 40, 50, 70, 120],\n",
    "    'ada__learning_rate': [1.0, 0.5, 0.3, 0.1]\n",
    "}\n",
    "\n",
    "ada_cv = GridSearchCV(\n",
    "    ada_pipeline,\n",
    "    ada_params,\n",
    "    cv=rskf,\n",
    "    scoring=[\"f1\", \"accuracy\"],\n",
    "    refit=\"f1\",\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1-score: 0.771\n",
      "\n",
      "Best parameter set: {'ada__estimator': DecisionTreeClassifier(max_depth=5, min_samples_leaf=0.05,\n",
      "                       min_samples_split=0.2, random_state=42), 'ada__learning_rate': 0.3, 'ada__n_estimators': 120}\n",
      "\n",
      "Scores:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92       549\n",
      "           1       0.90      0.82      0.86       342\n",
      "\n",
      "    accuracy                           0.90       891\n",
      "   macro avg       0.90      0.88      0.89       891\n",
      "weighted avg       0.90      0.90      0.89       891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ada_cv.fit(X, y)\n",
    "print(f\"Best F1-score: {ada_cv.best_score_:.3f}\\n\")\n",
    "print(f\"Best parameter set: {ada_cv.best_params_}\\n\")\n",
    "print(f\"Scores: {classification_report(y, ada_cv.predict(X))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Envio e considerações"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O melhor modelo treinado foi o Adaboost usando DecisionTree como modelo base, com os parâmetros achados anteriormente no treino da RF. Notando que estamos apenas usando os dados do dataset de treino, e faremos o teste real posteriormente com os envios pro Kaggle.\n",
    "\n",
    "Enviaremos os datasets de todos os modelos treinados para então comparar seus resultados de teste.\n",
    "\n",
    "Devido ao tempo relativamente curto, não foram testados outros modelos, ou mais hiper-parâmetros ainda. Além disso, poderia ser feita um melhor desenvolvimento na parte de Feature Engineering. Também poderia ser usado o MLflow para auxiliar nas comparações e armazenamento de informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         1\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         1\n",
       "..           ...       ...\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = test.loc[:, [\"PassengerId\"]]\n",
    "output[\"Survived\"] = rf_cv.predict(test)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_time = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "output_id = \"submission_rf_cvs_lp_\" + output_time + \".csv\"\n",
    "output_path = path.join(DATA_FOLDER, OUTPUT_FOLDER, output_id)\n",
    "output.to_csv(output_path, sep=\",\", index_label=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resultados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada um dos modelos foi enviado, com scores muito próximos entre 0.76 e 0.77. Adaboost foi melhor no treino, mas performou semelhante aos outros modelos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ddf9fd34b59fb13a4b9cd8f11e7719fb16373359848fa4ba3ca28ad9c242f09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
